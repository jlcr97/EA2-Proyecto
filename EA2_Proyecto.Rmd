---
title: "EA2_Proyecto"
output: html_document
---

```{r}
library(tidyverse)
library(dplyr)
library(tidyr)
library(matlib)
library(ggplot2)
#install.packages("olsrr"), es para las regresiones
library(olsrr)
#install.packates("GGally"), extensión de ggplot
#install.packages("GGally")
library(GGally)
library(MASS)
library(lmtest)
library(orcutt)
library(HoRM)
library(car)
```

Lectura de la base de datos 
```{r}
  #datos <- read.csv("BaseDatosEA2.csv") 
  #Cambiamos la base porque nos marcaba error en la linea 43, esta base ya esta limpia, solo estan los datos de Alvaro Obregon y las variables que
  #pensamos utilizar en el modelo
  datos <- read.csv("BaseDatos2.csv")
```

Número de renglones y nombre de las columnas 
```{r}
  nrow(datos) 
  names(datos)
```

Como ya tenemos la base de los datos de Alvaro Obregon y las variables que pensamos utilizar en el modelo ya no haremos este Chunk

ESTO YA NO ----------------------------------------------------------------------------------------------------------------
Seleccionamos las columnas que queremos trabajar y filtrando Avaro Obregon para nuestro análisis
```{r}
 #datos <- datos %>%  #CAUSA ERROR
  #select(sexo_jefe,edad_jefe,educa_jefe,tot_integ,perc_ocupa, ubica_geo, est_socio, ing_cor,gasto_mon,otros_gas,ahorro,deposito,retiro_inv,pago_tarje, tabaco, erogac_tot, 
         #deudas, vestido, transporte) %>% 
  #filter(ubica_geo > 9009, ubica_geo < 9011) ##Alvaro Obregon es el 9010
#Otras erogaciones: suma de depositos, pago de deudas, tarjetas de crédito, préstamos a otros
   #nrow(datos)
   #head(datos)
#Creamos una nueva columna para la edad al cuadrado
#datos = mutate(datos, edad_2=datos$edad_jefe^2)
#  head(datos)
```
--------------------------------------------------------------------------------

Optimizar la elección de variables para nuestro modelo de regresión lineal múltiple.


```{r}
#Regresión múltiple lineal entre todas las combinaciones posibles de 8 variables
modeloCDMX <- lm(datos$ing_cor ~ est_socio + perc_ocupa + erogac_tot + gasto_mon + sexo_jefe + educa_jefe + tot_integ + edad_jefe, data = datos)
summary(modeloCDMX)

#Optimiza y muestra las mejores 8 regresiones por número de variables a tomar en cuenta
ols_step_best_subset(modeloCDMX)

```

Analizando el modelo individual, obtendremos los valores estimados bo, .., b6 para cada variable explicativa. Así también un análisis general de los residuos.
```{r}

modeloCDMX <- lm(datos$ing_cor ~ edad_jefe + erogac_tot + gasto_mon + sexo_jefe + educa_jefe + est_socio, datos)
summary(modeloCDMX) 
```
Notemos que por el **p-value** y el **estadístico F**, se rechaza la hipótesis de que $\beta_0 \neq 0$, por lo que sí tenemos ordenada al origen.
También veamos que con las **pruebas t** rechazamos las variables de sexo del jefe del hogar y el estrato socioeconómico del mismo. Por lo tanto, solo vamos a considerar a $Y = \beta_0 + \beta_1 X_{edad} + \beta_2 X_{erogac} + \beta_4 X_{gasto} + \beta_5 X_{añosEsc}$


Renombramos las variables del sexo 1: hombre y 0: mujer
```{r}
  datos$sexo_jefe <- factor(datos$sexo_jefe)
  levels(datos$sexo_jefe) <- c("Hombre","Mujer")
```


Graficas de las variables independientes elegidas vs ingreso
```{r}
#AGREGUEN MÁS O PÓNGANLAS MÁS BONITAS SI GUSTAN
ggplot(datos,aes(edad_jefe, ing_cor))+geom_point() +labs(y= "Ingreso corriente", x = "Edad")
ggplot(datos,aes(erogac_tot, ing_cor))+geom_point() +labs(y= "Ingreso corriente", x = "Erogaciones totales")
ggplot(datos,aes(gasto_mon, ing_cor))+geom_point() +labs(y= "Ingreso corriente", x = "Gasto monetario")
ggplot(datos,aes(sexo_jefe, ing_cor))+geom_point() +labs(y= "Ingreso corriente", x = "Sexo")
ggplot(datos,aes(as.factor(educa_jefe), ing_cor))+geom_point() +labs(y= "Ingreso corriente", x = "Años escolarizados")
ggplot(datos,aes(as.factor(est_socio), ing_cor))+geom_point() +labs(y= "Ingreso corriente", x = "Estrato socioeconómico")
```


Tras reducir el modelo por dos variable; es decir, tras quitar el estrato socioeconómico y el sexo, el modelo mejorado queda como $Y = \beta_0 + \beta_1 X_{edad} + \beta_2 X_{erogac} + \beta_4 X_{gasto} + \beta_5 X_{añosEsc}$
```{r}
  mejoraCDMX <- lm(datos$ing_cor ~ edad_jefe  +erogac_tot + gasto_mon  + educa_jefe, datos) 
  summary(mejoraCDMX) 
```

Analizamos la tendencia de autocorrelación gráficamente:
```{r}
  plot(mejoraCDMX)
```


Ahora llevamos a cabo la prueba Durbin-Watson
Tenemos que n = 144, $\alpha$=0.05 y 5 términos considerando la ordenada al origen lo que nos da
$d_{L}$ = 1.67104 y $d_{U}$=1.78508

```{r}
  dwtest(mejoraCDMX)
```
Como $d_W$=1.6356 < 1.67104 = $d_L$, **sí** hay autocorrelación


Como efectivamente hay autocorrelación, llevamos a cabo el método de Cochrane-Orcutt para agregar un AR(1) en el modelo
```{r}
  #install.packages("orcutt")
  #library(orcutt)
  mCorregido <- cochrane.orcutt(mejoraCDMX)
  summary(mCorregido)
  mCorregido
```

Esto es para la **autocorrelación**
Notemos que con el nuevo modelo, ya no hay autocorrelación
```{r}
  dwtest(mCorregido)
```
pues $d_W$=1.9815 > 1.78508 = $d_U$

De esta manera, nuestro modelo estimado es 
$$\hat{Y} = -67285.185 + 997.229 X_{edad} + 0.701 X_{erogac} + 1.155 X_{gasto} + 5863.810 X_{añosEsc}$$

Para ver si hay **colinealidad** veamos si algunos de los eigenvalores de X'X son cercanos a 0
```{r}
  m <- as.matrix(datos)
  m<- m[,-c(1,2,5,6,7)]
  mat <- t(m) %*% m
  eigen(mat)
```
Observemos que el eigenvalor más pequeño es 1545.136 $\neq$ 0, por lo que no se sugiere que haya colinealidad. Saquemos el número de condición de la matriz dado por $k=\frac{\lambda_{max}}{\lambda_{min}}$
```{r}
  eigen(mat)$values[1] / eigen(mat)$values[4]
```

Para el número de condición de la matriz
```{r}
  
```




Gráfica de Residuos vs Y estimada
```{r}
ggplot(modeloCDMX, aes(x = modeloCDMX$fitted.values ,y = modeloCDMX$residuals)) + 
  labs(x = 'Ingreso Estimado',y = 'Residuos Estandarizados', title = 'Ingreso Estimado vs. Residuos') +
  geom_point() +
  theme_economist()
```

Para supuesto esperanza cero del error y NORMALIDAD DE LOS ERRORES:
```{r}
summary(mCorregido)
media_error <- sum(mCorregido$residuals)

S <-sqrt(sum(mCorregido$residuals^2)/(144-5))
resi_est <- mCorregido$residuals/S

ggplot(datos, aes(x=resi_est)) + geom_histogram() + theme_bw()

mCorregido %>% 
  ggplot(aes(x=resi_est)) + geom_density() + theme_bw()

install.packages("normtest")
library(normtest)
jb.norm.test(mCorregido$residuals,nrepl=2000) #Use Jarque Bera porque ya incluye lo de asimetría y kurtosis
#Se rechaza la hipótesis nula de que se distribuyen normal
```
TRANSFORMACION POTENCIA
```{r}
transf <- lm(log(ing_cor) ~ edad_jefe  + erogac_tot + gasto_mon  + educa_jefe, datos)
summary(transf)
S_tr<-sqrt(sum(transf$residuals^2)/139)
ggplot(datos, aes(transf$residuals/S_tr)) + geom_density() + theme_bw()
jb.norm.test(transf$residuals,nrepl=2000) 
```

HETEROSCEDASTICIDAD
```{r}
#primero analizamos la siguiente grafica
ggplot(datos, aes(x = mCorregido$fitted.values ,y = resi_est)) + 
  labs(x = 'Ingreso Estimado',y = 'Residuos Estandarizados', title = 'Ingreso Estimado vs. Residuos') + geom_point() + theme_bw()
#Vemos cómo que los residuos siguen un patron
#Para confirmar hacemos prueba de white
#Regresión Auxiliar
aux <- lm(mCorregido$residuals^2 ~ edad_jefe  + erogac_tot + gasto_mon  + educa_jefe + edad_jefe^2  + erogac_tot^2 + gasto_mon^2  + educa_jefe^2 + edad_jefe*erogac_tot + edad_jefe*gasto_mon  + edad_jefe*educa_jefe + erogac_tot*gasto_mon  + erogac_tot*educa_jefe + gasto_mon*educa_jefe, datos)
summary(aux)
#El estadistico de prueba es
nr2 <- 144*0.07749
qchisq(.95, 14)
#No se rechaza la hipótesis Ho, los residuos son homoscedasticos

```





