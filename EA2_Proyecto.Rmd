---
title: "Ingreso en Álvaro Obregón"
author: "Equipo 6"
output: html_document
---

```{r, warning=FALSE}
suppressPackageStartupMessages({
library(tidyverse)
library(dplyr)
library(tidyr)
library(matlib)
library(ggplot2)
library(olsrr)
library(GGally)
library(MASS)
library(lmtest)
library(orcutt)
library(HoRM)
library(normtest)
library(car)
library(nlme)
})
```

Lectura de la base de datos 
```{r}
  #datos <- read.csv("BaseDatosEA2.csv") 
  #Cambiamos la base porque nos marcaba error en la linea 43, esta base ya esta limpia, solo estan los datos de Alvaro Obregon y las variables que
  #pensamos utilizar en el modelo
  datos <- read.csv("BaseDatos2.csv")
  head(datos)
```

Número de renglones y nombre de las columnas 
```{r}
  nrow(datos) 
  names(datos)
```

Optimizar la elección de variables para nuestro modelo de regresión lineal múltiple.
```{r}
#Regresión múltiple lineal entre todas las combinaciones posibles de 8 variables
modeloCDMX <- lm(datos$ing_cor ~ est_socio + perc_ocupa + erogac_tot + gasto_mon + sexo_jefe + educa_jefe + tot_integ + edad_jefe, data = datos)
summary(modeloCDMX)

#Optimiza y muestra las mejores 8 regresiones por número de variables a tomar en cuenta
ols_step_best_subset(modeloCDMX)

```
Inicialmente elegimos el modelo con las 6 variables puesto que $R^2=0.7650$

Analizando el modelo individual, obtendremos los valores estimados $b_0, .., b_6$ para cada variable explicativa. Así también un análisis general de los residuos.
```{r}

modeloCDMX <- lm(datos$ing_cor ~ edad_jefe + erogac_tot + gasto_mon + sexo_jefe + educa_jefe + est_socio, datos)
summary(modeloCDMX) 
```

Como estrato socioeconómico es el p-value más grande, ahora corremos la regresión omitiendo esa variable y volvemos a buscar el p-value más grande
```{r}
  modeloCDMX2 <- lm(datos$ing_cor ~ edad_jefe + erogac_tot + gasto_mon + sexo_jefe + educa_jefe, datos)
  summary(modeloCDMX2) 
```

Notemos que por el **p-value** y el **estadístico F**, se rechaza la hipótesis de que $\beta_0 \neq 0$, por lo que sí tenemos ordenada al origen.
También veamos que con las **pruebas t** rechazamos las variables de sexo del jefe del hogar y el estrato socioeconómico del mismo.


Veamos que ahora con el modelo de regresión lineal solo con las 4 variables: edad, erogaciones totales, gasto y años escolarizados ya todas son significativas. Por lo tanto, solo vamos a considerar a $Y = \beta_0 + \beta_1 X_{edad} + \beta_2 X_{erogac} + \beta_4 X_{gasto} + \beta_5 X_{añosEsc}$
```{r}
    mejoraCDMX1 <- lm(datos$ing_cor ~ edad_jefe  +erogac_tot + gasto_mon  + educa_jefe, datos) 
    summary(mejoraCDMX1) 
```

Renombramos las variables del sexo 1: hombre y 0: mujer
```{r}
  datos$sexo_jefe <- factor(datos$sexo_jefe)
  levels(datos$sexo_jefe) <- c("Hombre","Mujer")
```

Renombramos las variables del estrato socioeconómico 1: bajo, 2: medio bajo, 3: medio alto, 4: alto
```{r}
  datos$est_socio<- factor(datos$est_socio)
  levels(datos$est_socio) <- c("Medio bajo", "Medio alto", "Alto")
```


Graficas de las variables independientes elegidas vs ingreso
```{r}
ggplot(datos,aes(edad_jefe, ing_cor))+geom_point() +labs(y= "Ingreso corriente", x = "Edad")
ggplot(datos,aes(erogac_tot, ing_cor))+geom_point() +labs(y= "Ingreso corriente", x = "Erogaciones totales")
ggplot(datos,aes(gasto_mon, ing_cor))+geom_point() +labs(y= "Ingreso corriente", x = "Gasto monetario")
ggplot(datos,aes(sexo_jefe, ing_cor))+geom_point() +labs(y= "Ingreso corriente", x = "Sexo")
ggplot(datos,aes(as.factor(educa_jefe), ing_cor))+geom_point() +labs(y= "Ingreso corriente", x = "Años escolarizados")
ggplot(datos,aes(as.factor(est_socio), ing_cor))+geom_point() +labs(y= "Ingreso corriente", x = "Estrato socioeconómico")
```

ESTE ES EL MODELO CON EL QUE VAMOS A TRABAJAR 

Tras reducir el modelo por dos variable; es decir, tras quitar el estrato socioeconómico y el sexo, el modelo mejorado queda como $Y = \beta_0 + \beta_1 X_{edad} + \beta_2 X_{erogac} + \beta_3 X_{gasto} + \beta_4 X_{añosEsc}$
```{r}
  mejoraCDMX <- lm(datos$ing_cor ~ edad_jefe  +erogac_tot + gasto_mon  + educa_jefe, datos) 
  summary(mejoraCDMX) 
```
Entonces el modelo corregido, que en con el que nos quedamos es 
$$Y = -74450 + 998.4 X_{edad} + 0.6726 X_{erogac} + 1.228 X_{gasto} + 9390 X_{añosEsc}$$

Para ver si hay **colinealidad** veamos si algunos de los eigenvalores de X'X son cercanos a 0
```{r}
  m <- datos
  m<- m[,-c(1,2,5,6,7)]
  m1 <- as.matrix(m)
  mat <- t(m1) %*% m1
  eigen(mat)
```

Observemos que el eigenvalor más pequeño es 1545.136 $\neq$ 0, por lo que no se sugiere que haya colinealidad. Saquemos el número de condición de la matriz dado por $k=\frac{\lambda_{max}}{\lambda_{min}}$
```{r}
  eigen(mat)$values[1] / eigen(mat)$values[4]
```




Para supuesto esperanza cero del error y NORMALIDAD DE LOS ERRORES:
```{r}
summary(mejoraCDMX)
media_error <- sum(mejoraCDMX$residuals)

S <-sqrt(sum(mejoraCDMX$residuals^2)/(144-5))
resi_est <- mejoraCDMX$residuals/S
#primero cargamos la base de datos sin los datos atipicos
BaseSA <- read.csv("BaseSA.csv")
sin_ati <- lm(BaseSA$ing_cor ~ edad_jefe  +erogac_tot + gasto_mon  + educa_jefe, BaseSA) 
summary(sin_ati) 
ggplot(BaseSA, aes( x = sin_ati$residuals)) + labs(x = 'Residuos',y = 'Frecuencia', title = '') + geom_histogram() + theme_bw()

ggplot(BaseSA,aes(sin_ati$residuals))+ labs(x = 'Residuos',y = 'Densidad', title = '') + geom_density() + theme_bw()

jb.norm.test(sin_ati$residuals,nrepl=2000) #Use Jarque Bera porque ya incluye lo de asimetría y kurtosis
#Se rechaza la hipótesis nula de que se distribuyen normal
```

Histograma con línea de modelo inicial
```{r}
  #primero cargamos la base de datos sin los datos atipicos
  BaseSA <- read.csv("BaseSA.csv")
  sin_ati <- lm(BaseSA$ing_cor ~ edad_jefe  +erogac_tot + gasto_mon  + educa_jefe, BaseSA) 
  
  X <- sin_ati$residuals
  df <- data.frame(X) 
  histog <- ggplot(data = df, aes(x=X)) + geom_histogram(aes(y=..density..), color = "black") + theme_bw() + geom_density(kernel="gaussian", color = "red", size=2) + labs(x = 'Residuos',y = 'Frecuencia relativa', title = '')
  histog
  
```



TRANSFORMACION POTENCIA

```{r}

transf <- lm(ing_cor^(1/2) ~ edad_jefe  + erogac_tot + gasto_mon  + educa_jefe, BaseSA)
summary(transf)
St<-sqrt(sum(transf$residuals^2)/139)
ggplot(BaseSA, aes( x = transf$residuals)) + labs(x = 'Residuos',y = 'Frecuencia', title = '') + geom_histogram() + theme_bw()

ggplot(BaseSA,aes(transf$residuals))+ labs(x = 'Residuos',y = 'Densidad', title = '') + geom_density() + theme_bw()
jb.norm.test(transf$residuals,nrepl=2000) 
#comparamos con
qchisq(.95,2)
#No se rechaza la hipotesis nula de que se distribuyen normal
```

Histograma con densidad de transformación potencia

```{r}
  
  transf <- lm(ing_cor^(1/2) ~ edad_jefe  + erogac_tot + gasto_mon  + educa_jefe, BaseSA)
  
  X <- transf$residuals
  df <- data.frame(X) 
  histog <- ggplot(data = df, aes(x=X)) + geom_histogram(aes(y=..density..), color = "black") + theme_bw() + geom_density(kernel="gaussian", color = "red", size=2) + labs(x = 'Residuos',y = 'Frecuencia relativa', title = '')
  histog
  
```




HETEROSCEDASTICIDAD
```{r}
#primero analizamos la siguiente grafica modelo solo con orcutt
ggplot(datos, aes(x = mejoraCDMX$fitted.values ,y =mejoraCDMX$residuals/S)) + 
  labs(x = 'Ingreso Estimado',y = 'Residuos Estandarizados', title = '') + geom_point() + theme_bw()
#Vemos cómo que los residuos siguen un patron
#Regresión Auxiliar
aux <- lm(mejoraCDMX$residuals^2 ~ edad_jefe  + erogac_tot + gasto_mon  + educa_jefe + edad_jefe^2  + erogac_tot^2 + gasto_mon^2  + educa_jefe^2 + edad_jefe*erogac_tot + edad_jefe*gasto_mon  + edad_jefe*educa_jefe + erogac_tot*gasto_mon  + erogac_tot*educa_jefe + gasto_mon*educa_jefe, datos)
summary(aux)
nr2 <- 144*0.07631
nr2

#analizando sin datos atipicos
Sta<-sqrt(sum(sin_ati$residuals^2)/139)
ggplot(BaseSA, aes(x = sin_ati$fitted.values ,y =sin_ati$residuals/Sta)) + 
  labs(x = 'Ingreso Estimado',y = 'Residuos Estandarizados', title = '') + geom_point() + theme_bw()
aux2 <- lm(sin_ati$residuals^2 ~ edad_jefe  + erogac_tot + gasto_mon  + educa_jefe + edad_jefe^2  + erogac_tot^2 + gasto_mon^2  + educa_jefe^2 + edad_jefe*erogac_tot + edad_jefe*gasto_mon  + edad_jefe*educa_jefe + erogac_tot*gasto_mon  + erogac_tot*educa_jefe + gasto_mon*educa_jefe, BaseSA)
summary(aux2)
nr22 <-144*0.06803
nr22

#Analizando heterosc con la transformcion potencia
ggplot(BaseSA, aes(x = transf$fitted.values ,y =transf$residuals/St)) + 
  labs(x = 'Ingreso Estimado',y = 'Residuos Estandarizados', title = '') + geom_point() + theme_bw()
#prueba de white
#Regresión Auxiliar
aux3 <- lm(transf$residuals^2 ~ edad_jefe  + erogac_tot + gasto_mon  + educa_jefe + edad_jefe^2  + erogac_tot^2 + gasto_mon^2  + educa_jefe^2 + edad_jefe*erogac_tot + edad_jefe*gasto_mon  + edad_jefe*educa_jefe + erogac_tot*gasto_mon  + erogac_tot*educa_jefe + gasto_mon*educa_jefe, BaseSA)
summary(aux3)
#El estadistico de prueba es
nr23 <- 144*0.1247
nr23
qchisq(.95, 14)
#No se rechaza la hipótesis Ho, los residuos son homoscedasticos

```





